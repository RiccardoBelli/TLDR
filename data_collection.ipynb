{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = 'data/emails.txt'\n",
    "with open(path, 'rb') as f:\n",
    "    text = f.read().decode('utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(dirty_text):\n",
    "    # The following pattern will match any character that is not a letter, number, basic punctuation, or `-` and ``\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s.,?!:;@#$%^&*()_+={}[\\]|\\\\<>/\"\\'`-]', flags=re.UNICODE)\n",
    "    clean_text = pattern.sub('', dirty_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_minute_read(dirty_text):\n",
    "    minute_read_pattern = re.compile(r'\\(?\\d*\\s*minute\\s*read\\)?', re.IGNORECASE)\n",
    "    clean_text = re.sub(minute_read_pattern, '', dirty_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import quopri\n",
    "\n",
    "def decode_quoted_printable(coded_text):\n",
    "    decoded_text = quopri.decodestring(coded_text.encode('utf-8', errors='replace')).decode('utf-8', errors='replace')\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_square_brackets_urls(dirty_text):\n",
    "    cleaned_text = re.sub(r'\\[.*?\\]', '', dirty_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_blank_lines(dirty_text):\n",
    "    lines = dirty_text.splitlines()\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cleanup_text(dirty_text):\n",
    "    dirty_text = decode_quoted_printable(dirty_text)\n",
    "    dirty_text = remove_special_characters(dirty_text)\n",
    "    dirty_text = remove_square_brackets_urls(dirty_text)\n",
    "    dirty_text = remove_minute_read(dirty_text)\n",
    "    clean_text = remove_blank_lines(dirty_text)\n",
    "    return clean_text\n",
    "\n",
    "text = cleanup_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_sections(text, start_section, end_section):\n",
    "    # Define the regular expression pattern to match the desired sections\n",
    "    pattern = re.compile(r'(?i){}(.*?)(?={})'.format(re.escape(start_section), re.escape(end_section)), re.MULTILINE | re.DOTALL)\n",
    "    sections = re.findall(pattern, text)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arguments = [\n",
    "    \"Big Tech & Startups\",\n",
    "    \"Science & Futuristic Technology\",\n",
    "    \"Programming, Design & Data Science\",\n",
    "    \"Miscellaneous\"\n",
    "]\n",
    "\n",
    "argument_to_code = {argument: code for code, argument in enumerate(arguments)}\n",
    "\n",
    "text_by_argument = {}\n",
    "\n",
    "for i in range(len(arguments) - 1):\n",
    "    argument = arguments[i]\n",
    "    next_argument = arguments[i + 1]\n",
    "\n",
    "    text_by_argument[argument] = extract_sections(text, argument, next_argument)\n",
    "\n",
    "for section, content in text_by_argument.items():\n",
    "    print(f\"{section}: {len(content)} sections extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_first_10_sections(sections):\n",
    "    for i, section in enumerate(sections[:10], start=1):\n",
    "        print(section)\n",
    "\n",
    "#print the first 10 extracted section for the BIG TECH & STARTUPS argument\n",
    "print_first_10_sections(text_by_argument[arguments[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def split_section_into_articles(section):\n",
    "    pattern = r\"\\.(?=\\s*(?:\\n|\\r\\n?)*\\s*[A-Z]{2,}(?:\\s[A-Z]+)*(?!\\w))\"\n",
    "    articles = re.split(pattern, section)\n",
    "    return [article.strip() for article in articles if article.strip()]\n",
    "\n",
    "def create_articles_dataframe(text_by_argument):\n",
    "    data = []\n",
    "\n",
    "    for argument, sections in text_by_argument.items():\n",
    "        category_code = argument_to_code[argument]\n",
    "        sections = text_by_argument[argument]\n",
    "        for section in sections:\n",
    "            articles = split_section_into_articles(section)\n",
    "            for article in articles:\n",
    "                article = article.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                data.append((article, category_code, argument))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"text\", \"category_code\", \"category\"])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles_df = create_articles_dataframe(text_by_argument)\n",
    "articles_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_title(article_text):\n",
    "    title_pattern = r'^[^a-z]+'\n",
    "    title_search = re.search(title_pattern, article_text)\n",
    "\n",
    "    title = title_search.group().strip()\n",
    "    last_char_of_title = title[-1]\n",
    "    title = title[:-1]\n",
    "    article_text = last_char_of_title + article_text[len(title_search.group()):].strip()\n",
    "\n",
    "    return title, article_text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, download\n",
    "\n",
    "download('punkt')\n",
    "\n",
    "def sentence_segmentation(article_text):\n",
    "    title, article_text = extract_title(article_text)\n",
    "    all_sentences = [title]\n",
    "\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    for sentence in sentences:\n",
    "        all_sentences = all_sentences + sentence.split('  ')\n",
    "\n",
    "    return [x.strip() for x in all_sentences]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_sentences_dataframe(articles_df):\n",
    "    data = []\n",
    "\n",
    "    for _, row in articles_df.iterrows():\n",
    "        article = row['text']\n",
    "        category_code = row['category_code']\n",
    "        category = row['category']\n",
    "        sentences = sentence_segmentation(article)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            data.append((sentence, category_code, category))\n",
    "\n",
    "    df_sentences = pd.DataFrame(data, columns=[\"text\", \"category_code\", \"category\"])\n",
    "    return df_sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences_df = create_sentences_dataframe(articles_df)\n",
    "sentences_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_save_dataframes(df, name, train_ratio=0.7, validation_ratio=0.15, test_ratio=0.15):\n",
    "    assert (train_ratio + validation_ratio + test_ratio == 1), \"Ratios must sum up to 1\"\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, stratify=df['category_code'], train_size=train_ratio)\n",
    "    validation_df, test_df = train_test_split(temp_df, stratify=temp_df['category_code'], train_size=(validation_ratio / (validation_ratio + test_ratio)))\n",
    "\n",
    "    train_df.to_pickle(f'data/{name}_training.pkl')\n",
    "    validation_df.to_pickle(f'data/{name}_validation.pkl')\n",
    "    test_df.to_pickle(f'data/{name}_test.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles_df.to_pickle('data/articles_df.pkl')\n",
    "sentences_df.to_pickle('data/sentences_df.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "split_and_save_dataframes(articles_df, 'articles')\n",
    "split_and_save_dataframes(sentences_df, 'sentences')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
