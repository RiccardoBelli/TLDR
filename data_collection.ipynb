{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TLDR\n",
    "### by Riccardo Belli, Daniel Grgic Figueiredo, Luca Rausa\n",
    "TLDR (*_Too Long, Didn't Read_*) is a daily newsletter that offers brief and easily digestible summaries of the most important news stories of the day. The newsletter covers a wide range of topics, including technology, business, politics, and culture. Its aim is to provide busy professionals and individuals with a quick and informative overview of current events, without the need to sift through lengthy articles. The newsletter's summaries are written in an engaging style, making it accessible to readers of all backgrounds.\n",
    "\n",
    "We chose this dataset due to the lengthy list of articles at our disposable, but also due to the well structured and well separated topics which although similar in topic still display some rather noticeable differences, meaning there is a relevant amount of data to work with, while presenting a challenge to our study due to the intersections of the main topics selected.\n",
    "\n",
    "The main topics mentioned are: \"Big Tech & Startups\", a dedicated section for big technology companies and their news as well as relevant startup companies that might have a chance to compete with these companies or that have tried to do so; \"Science & Futuristic Technology\", although this section is often dedicated to medical news, space travel and lifestyle technology (food/drink related articles and more) some of the terms may intersect with the other 2 topics, but still the one that has the least to do with others; \"Programming, Design & Data Science\", this last topic often includes github repositories as well as big news for programming languages as well as famous libraries, articles in this topic may some times collide with \"Big Tech\" due to the impact big tech companies have on programming languages and commonly used libraries.\n",
    "\n",
    "\n",
    "<sub>If you're interested in subscribing to TLDR, you can do so by visiting their website at https://tldr.tech.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "path = 'data/emails.txt'\n",
    "with open(path, 'rb') as f:\n",
    "    text = f.read().decode('utf-8', errors='replace')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the mess\n",
    "The first step after downloading the required e-mails sent to a private account was to clean the text and make sure that there were no special characters that would interfere with our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_characters(dirty_text):\n",
    "    # The following pattern will match any character that is not a letter, number, basic punctuation, or `-` and ``\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s.,?!:;@#$%^&*()_+={}[\\]|\\\\<>/\"\\'`-]', flags=re.UNICODE)\n",
    "    clean_text = pattern.sub('', dirty_text)\n",
    "    return clean_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection (5 minute read)\n",
    "All articles available have a title most of which are followed by an estimated time that an average person would take to read the full article. So we thought to remove it, in order to clean up a bit of the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_minute_read(dirty_text):\n",
    "    minute_read_pattern = re.compile(r'\\(?\\d*\\s*minute\\s*read\\)?', re.IGNORECASE)\n",
    "    clean_text = re.sub(minute_read_pattern, '', dirty_text)\n",
    "    return clean_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaner than ever\n",
    "Lastly the articles were further cleaned in a more detailed way, removing any brackets, empty lines and any other elements that were in the initial file, but were not a part of the text to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import quopri\n",
    "\n",
    "def decode_quoted_printable(coded_text):\n",
    "    decoded_text = quopri.decodestring(coded_text.encode('utf-8', errors='replace')).decode('utf-8', errors='replace')\n",
    "    return decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_square_brackets_urls(dirty_text):\n",
    "    cleaned_text = re.sub(r'\\[.*?\\]', '', dirty_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_blank_lines(dirty_text):\n",
    "    lines = dirty_text.splitlines()\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "    cleaned_text = '\\n'.join(cleaned_lines)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def cleanup_text(dirty_text):\n",
    "    dirty_text = decode_quoted_printable(dirty_text)\n",
    "    dirty_text = remove_special_characters(dirty_text)\n",
    "    dirty_text = remove_square_brackets_urls(dirty_text)\n",
    "    dirty_text = remove_minute_read(dirty_text)\n",
    "    clean_text = remove_blank_lines(dirty_text)\n",
    "    return clean_text\n",
    "\n",
    "text = cleanup_text(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction ready\n",
    "Once the text is all clean, the sections can be extracted, because the newsletter has a regular structure, we can use a _start section_ string and an _end section_ string which are used to separate the texts in a given email based on their topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_sections(text, start_section, end_section):\n",
    "    # Define the regular expression pattern to match the desired sections\n",
    "    pattern = re.compile(r'(?i){}(.*?)(?={})'.format(re.escape(start_section), re.escape(end_section)), re.MULTILINE | re.DOTALL)\n",
    "    sections = re.findall(pattern, text)\n",
    "\n",
    "    return sections"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big three and another one\n",
    "The newsletter has the following arguments in the given order: [Big Tech & Startups, Science & Futuristic Technology, Programming, Design & Data Science] and these were the ones we used for our classification as a last argument we added _Miscellaneous_ to point as to where _Programming, Design & Data Science_ would end.\n",
    "Once these were established we can proceed to the separation by arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "arguments = [\n",
    "    \"Big Tech & Startups\",\n",
    "    \"Science & Futuristic Technology\",\n",
    "    \"Programming, Design & Data Science\",\n",
    "    \"Miscellaneous\"\n",
    "]\n",
    "\n",
    "argument_to_code = {argument: code for code, argument in enumerate(arguments)}\n",
    "\n",
    "text_by_argument = {}\n",
    "\n",
    "for i in range(len(arguments) - 1):\n",
    "    argument = arguments[i]\n",
    "    next_argument = arguments[i + 1]\n",
    "\n",
    "    text_by_argument[argument] = extract_sections(text, argument, next_argument)\n",
    "\n",
    "for section, content in text_by_argument.items():\n",
    "    print(f\"{section}: {len(content)} sections extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_first_10_sections(sections):\n",
    "    for i, section in enumerate(sections[:10], start=1):\n",
    "        print(section)\n",
    "\n",
    "#print the first 10 extracted section for the BIG TECH & STARTUPS argument\n",
    "print_first_10_sections(text_by_argument[arguments[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From sections to single articles\n",
    "Once the sections have been properly separated, they are then further broken down into the articles. Each section contains at least 2 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def split_section_into_articles(section):\n",
    "    pattern = r\"\\.(?=\\s*(?:\\n|\\r\\n?)*\\s*[A-Z]{2,}(?:\\s[A-Z]+)*(?!\\w))\"\n",
    "    articles = re.split(pattern, section)\n",
    "    return [article.strip() for article in articles if article.strip()]\n",
    "\n",
    "def create_articles_dataframe(text_by_argument):\n",
    "    data = []\n",
    "\n",
    "    for argument, sections in text_by_argument.items():\n",
    "        category_code = argument_to_code[argument]\n",
    "        sections = text_by_argument[argument]\n",
    "        for section in sections:\n",
    "            articles = split_section_into_articles(section)\n",
    "            for article in articles:\n",
    "                article = article.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                data.append((article, category_code, argument))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"text\", \"category_code\", \"category\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "articles_df = create_articles_dataframe(text_by_argument)\n",
    "articles_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last but not least\n",
    "Once we have the articles separated, we can start creating the data_frames containing title, text and category of each article. This will allow us to then start working with the data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_title(article_text):\n",
    "    title_pattern = r'^[^a-z]+'\n",
    "    title_search = re.search(title_pattern, article_text)\n",
    "\n",
    "    title = title_search.group().strip()\n",
    "    last_char_of_title = title[-1]\n",
    "    title = title[:-1]\n",
    "    article_text = last_char_of_title + article_text[len(title_search.group()):].strip()\n",
    "\n",
    "    return title, article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, download\n",
    "\n",
    "download('punkt')\n",
    "\n",
    "def sentence_segmentation(article_text):\n",
    "    title, article_text = extract_title(article_text)\n",
    "    all_sentences = [title]\n",
    "\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    for sentence in sentences:\n",
    "        all_sentences = all_sentences + sentence.split('  ')\n",
    "\n",
    "    return [x.strip() for x in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_sentences_dataframe(articles_df):\n",
    "    data = []\n",
    "\n",
    "    for _, row in articles_df.iterrows():\n",
    "        article = row['text']\n",
    "        category_code = row['category_code']\n",
    "        category = row['category']\n",
    "        sentences = sentence_segmentation(article)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            data.append((sentence, category_code, category))\n",
    "\n",
    "    df_sentences = pd.DataFrame(data, columns=[\"text\", \"category_code\", \"category\"])\n",
    "    return df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sentences_df = create_sentences_dataframe(articles_df)\n",
    "sentences_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train(, validation and test) makes perfect\n",
    "With the dataframes properly set we want to further break them down into 3 seperate dataframes: train, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_save_dataframes(df, name, train_ratio=0.7, validation_ratio=0.15, test_ratio=0.15):\n",
    "    assert (train_ratio + validation_ratio + test_ratio == 1), \"Ratios must sum up to 1\"\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, stratify=df['category_code'], train_size=train_ratio)\n",
    "    validation_df, test_df = train_test_split(temp_df, stratify=temp_df['category_code'], train_size=(validation_ratio / (validation_ratio + test_ratio)))\n",
    "\n",
    "    train_df.to_pickle(f'data/{name}_training.pkl')\n",
    "    validation_df.to_pickle(f'data/{name}_validation.pkl')\n",
    "    test_df.to_pickle(f'data/{name}_test.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub> We save the dataframes containing articles (as well as one containing sentences) into files to more easily access this information late in a separate notebook, if needed. </sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "articles_df.to_pickle('data/articles_df.pkl')\n",
    "sentences_df.to_pickle('data/sentences_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "split_and_save_dataframes(articles_df, 'articles')\n",
    "split_and_save_dataframes(sentences_df, 'sentences')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
